{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNSV4QGHS1I1"
      },
      "source": [
        "# **Homework 2-2 Hessian Matrix**\n",
        "\n",
        "* Slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW02/HW02.pdf\n",
        "* Video (Chinese): https://youtu.be/PdjXnQbu2zo\n",
        "* Video (English): https://youtu.be/ESRr-VCykBs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0eNH3RD73Ye"
      },
      "source": [
        "## Hessian Matrix\n",
        "Imagine we are training a neural network and we are trying to find out whether the model is at **local minima like, saddle point, or none of the above**. We can make our decision by calculating the Hessian matrix.\n",
        "\n",
        "In practice, it is really hard to find a point where the gradient equals zero or all of the eigenvalues in Hessian matrix are greater than zero. In this homework, we make the following two assumptions:\n",
        "1. View gradient norm less than 1e-3 as **gradient equals to zero**.\n",
        "2. If minimum ratio is greater than 0.5 and gradient norm is less than 1e-3, then we assume that the model is at “local minima like”.\n",
        "\n",
        "> Minimum ratio is defined as the proportion of positive eigenvalues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lezCgM8U8KJl"
      },
      "source": [
        "## IMPORTANT NOTICE\n",
        "In this homework, students with different student IDs will get different answers. Make sure to fill in your `student_id` in the following block correctly. Otherwise, your code may not run correctly and you will get a wrong answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bbr6MTQ488GH"
      },
      "outputs": [],
      "source": [
        "student_id = '2110zjx' # fill with your student ID\n",
        "\n",
        "assert student_id != 'your_student_id', 'Please fill in your student_id before you start.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHz08Ybg-dmB"
      },
      "source": [
        "## Calculate Hessian Matrix\n",
        "The computation of Hessian is done by TA, you don't need to and shouldn't change the following code. The only thing you need to do is to run the following blocks and determine whether the model is at `local minima like`, `saddle point`, or `none of the above` according to the value of `gradient norm` and `minimum ratio`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDeNevCUvDW"
      },
      "source": [
        "### Install Package to Compute Hessian.\n",
        "\n",
        "The autograd-lib library is used to compute Hessian matrix. You can check the full document here https://github.com/cybertronai/autograd-lib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r135K45psHwF",
        "outputId": "8492ab08-8ad9-4525-db9c-35884eaa0641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autograd-lib\n",
            "  Downloading autograd_lib-0.0.7-py3-none-any.whl (9.2 kB)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 533 kB/s eta 0:00:01\n",
            "\u001b[?25hCollecting seaborn\n",
            "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
            "\u001b[K     |████████████████████████████████| 292 kB 2.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting gin-config\n",
            "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 3.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from pytorch-lightning->autograd-lib) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from pytorch-lightning->autograd-lib) (1.21.2)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 2.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 2.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 2.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from pytorch-lightning->autograd-lib) (3.10.0.2)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[K     |████████████████████████████████| 701 kB 2.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 2.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from pytorch-lightning->autograd-lib) (1.7.1)\n",
            "Collecting tensorboard>=2.2.0\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from pytorch-lightning->autograd-lib) (4.63.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 2.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning->autograd-lib) (3.0.4)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
            "\u001b[K     |████████████████████████████████| 289 kB 2.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 2.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 2.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting protobuf>=3.6.0\n",
            "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 2.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning->autograd-lib) (0.37.0)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 2.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning->autograd-lib) (1.16.0)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 2.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->autograd-lib) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->autograd-lib) (3.6.0)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 2.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->autograd-lib) (2021.10.8)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 2.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 2.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 2.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 2.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->autograd-lib) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 2.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting scipy>=1.0\n",
            "  Downloading scipy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.6 MB 2.2 MB/s eta 0:00:01     |██████████████████████████▏     | 34.0 MB 2.6 MB/s eta 0:00:03\n",
            "\u001b[?25hCollecting pandas>=0.23\n",
            "  Downloading pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.7 MB 2.4 MB/s eta 0:00:01    |████████████▉                   | 4.7 MB 2.1 MB/s eta 0:00:04\n",
            "\u001b[?25hCollecting matplotlib>=2.2\n",
            "  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 2.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn->autograd-lib) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/jinxinzhu/programsoft/anaconda3/envs/torch_env/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn->autograd-lib) (2.8.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.31.1-py3-none-any.whl (899 kB)\n",
            "\u001b[K     |████████████████████████████████| 899 kB 2.1 MB/s eta 0:00:01    |███                             | 81 kB 2.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 2.4 MB/s eta 0:00:01\n",
            "\u001b[?25hBuilding wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=e20ea284bd6d19427291749869b3306ddd8a7a81ffc465603e909bd18f7d1fe7\n",
            "  Stored in directory: /home/jinxinzhu/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
            "Successfully built future\n",
            "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, rsa, requests, pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, google-auth, async-timeout, aiosignal, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pytz, pyDeprecate, protobuf, markdown, kiwisolver, grpcio, google-auth-oauthlib, fsspec, fonttools, cycler, aiohttp, absl-py, torchmetrics, tensorboard, scipy, PyYAML, pandas, matplotlib, future, seaborn, pytorch-lightning, gin-config, autograd-lib\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 58.0.4\n",
            "    Uninstalling setuptools-58.0.4:\n",
            "      Successfully uninstalled setuptools-58.0.4\n",
            "Successfully installed PyYAML-6.0 absl-py-1.0.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 autograd-lib-0.0.7 cachetools-5.0.0 charset-normalizer-2.0.12 cycler-0.11.0 fonttools-4.31.1 frozenlist-1.3.0 fsspec-2022.2.0 future-0.18.2 gin-config-0.5.0 google-auth-2.6.2 google-auth-oauthlib-0.4.6 grpcio-1.44.0 idna-3.3 kiwisolver-1.4.0 markdown-3.3.6 matplotlib-3.5.1 multidict-6.0.2 oauthlib-3.2.0 pandas-1.4.1 protobuf-3.19.4 pyDeprecate-0.3.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch-lightning-1.5.10 pytz-2021.3 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 scipy-1.8.0 seaborn-0.11.2 setuptools-59.5.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.7.2 urllib3-1.26.9 werkzeug-2.0.3 yarl-1.7.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install autograd-lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFGBCIFmVLS_"
      },
      "source": [
        "### Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_-vjBvH0uqA-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import pi\n",
        "from collections import defaultdict\n",
        "from autograd_lib import autograd_lib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubbsl4dUVUj6"
      },
      "source": [
        "### Define NN Model\n",
        "The NN model here is used to fit a single variable math function.\n",
        "$$f(x) = \\frac{\\sin(5\\pi x)}{5\\pi x}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uvdOpR9lVaJQ"
      },
      "outputs": [],
      "source": [
        "class MathRegressor(nn.Module):\n",
        "    def __init__(self, num_hidden=128):\n",
        "        super().__init__()\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(1, num_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.regressor(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nO0POKbWU9o"
      },
      "source": [
        "### Get Pretrained Checkpoints\n",
        "The pretrained checkpoints is done by TA. Each student will get a different checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUG_tQKLbIKB",
        "outputId": "46ab3391-f669-45cf-b3ae-a3a6ad901f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/sh: gdown: command not found\n"
          ]
        }
      ],
      "source": [
        "#!gdown --id 1ym6G7KKNkbsqSnMmnxdQKHO1JBoF0LPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOFibHKCek_A"
      },
      "source": [
        "### Load Pretrained Checkpoints and Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zkLZoCR51D7P"
      },
      "outputs": [],
      "source": [
        "# find the key from student_id\n",
        "import re\n",
        "\n",
        "key = student_id[-1]\n",
        "if re.match('[0-9]', key) is not None:\n",
        "    key = int(key)\n",
        "else:\n",
        "    key = ord(key) % 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OSU8vnXEbY6q"
      },
      "outputs": [],
      "source": [
        "# load checkpoint and data corresponding to the key\n",
        "model = MathRegressor()\n",
        "autograd_lib.register(model)\n",
        "\n",
        "data = torch.load('data.pth')[key]\n",
        "model.load_state_dict(data['model'])\n",
        "train, target = data['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyBX5Gvvm_IW"
      },
      "source": [
        "### Function to compute gradient norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2i8qGj2dnYBN"
      },
      "outputs": [],
      "source": [
        "# function to compute gradient norm\n",
        "def compute_gradient_norm(model, criterion, train, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    output = model(train)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "\n",
        "    grads = []\n",
        "    for p in model.regressor.children():\n",
        "        if isinstance(p, nn.Linear):\n",
        "            param_norm = p.weight.grad.norm(2).item()\n",
        "            grads.append(param_norm)\n",
        "\n",
        "    grad_mean = np.mean(grads) # compute mean of gradient norms\n",
        "\n",
        "    return grad_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSHRU6saoOnf"
      },
      "source": [
        "### Function to compute minimum ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zizIq6Y_o_UK"
      },
      "outputs": [],
      "source": [
        "# source code from the official document https://github.com/cybertronai/autograd-lib\n",
        "\n",
        "# helper function to save activations\n",
        "def save_activations(layer, A, _):\n",
        "    '''\n",
        "    A is the input of the layer, we use batch size of 6 here\n",
        "    layer 1: A has size of (6, 1)\n",
        "    layer 2: A has size of (6, 128)\n",
        "    '''\n",
        "    activations[layer] = A\n",
        "\n",
        "# helper function to compute Hessian matrix\n",
        "def compute_hess(layer, _, B):\n",
        "    '''\n",
        "    B is the backprop value of the layer\n",
        "    layer 1: B has size of (6, 128)\n",
        "    layer 2: B ahs size of (6, 1)\n",
        "    '''\n",
        "    A = activations[layer]\n",
        "    BA = torch.einsum('nl,ni->nli', B, A) # do batch-wise outer product\n",
        "\n",
        "    # full Hessian\n",
        "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA) # do batch-wise outer product, then sum over the batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l0r4R_-soT58"
      },
      "outputs": [],
      "source": [
        "# function to compute the minimum ratio\n",
        "def compute_minimum_ratio(model, criterion, train, target):\n",
        "    model.zero_grad()\n",
        "    # compute Hessian matrix\n",
        "    # save the gradient of each layer\n",
        "    with autograd_lib.module_hook(save_activations):\n",
        "        output = model(train)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "    # compute Hessian according to the gradient value stored in the previous step\n",
        "    with autograd_lib.module_hook(compute_hess):\n",
        "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
        "\n",
        "    layer_hess = list(hess.values())\n",
        "    minimum_ratio = []\n",
        "\n",
        "    # compute eigenvalues of the Hessian matrix\n",
        "    for h in layer_hess:\n",
        "        size = h.shape[0] * h.shape[1]\n",
        "        h = h.reshape(size, size)\n",
        "        h_eig = torch.symeig(h).eigenvalues # torch.symeig() returns eigenvalues and eigenvectors of a real symmetric matrix\n",
        "        num_greater = torch.sum(h_eig > 0).item()\n",
        "        minimum_ratio.append(num_greater / len(h_eig))\n",
        "\n",
        "    ratio_mean = np.mean(minimum_ratio) # compute mean of minimum ratio\n",
        "\n",
        "    return ratio_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABZhFwVZY3x3"
      },
      "source": [
        "### Mathematical Derivation\n",
        "\n",
        "Method used here: https://en.wikipedia.org/wiki/Gauss–Newton_algorithm\n",
        "\n",
        "> **Notations** \\\\\n",
        "> $\\mathbf{A}$: the input of the layer. \\\\\n",
        "> $\\mathbf{B}$: the backprop value. \\\\\n",
        "> $\\mathbf{Z}$: the output of the layer. \\\\\n",
        "> $L$: the total loss, mean squared error was used here, $L=e^2$. \\\\\n",
        "> $w$: the weight value.\n",
        "\n",
        "Assume that the input dimension of the layer is $n$, and the output dimension of the layer is $m$.\n",
        "\n",
        "The derivative of the loss is\n",
        "\n",
        "\\begin{align*}\n",
        "    \\left(\\frac{\\partial L}{\\partial w}\\right)_{nm} &= \\mathbf{A}_m \\mathbf{B}_n,\n",
        "\\end{align*}\n",
        "\n",
        "which can be written as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\frac{\\partial L}{\\partial w} &= \\mathbf{B} \\times \\mathbf{A}.\n",
        "\\end{align*}\n",
        "\n",
        "The Hessian can be derived as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathbf{H}_{ij}&=\\frac{\\partial^2 L}{\\partial w_i \\partial w_j} \\\\\n",
        "    &= \\frac{\\partial}{\\partial w_i}\\left(\\frac{\\partial L}{\\partial w_j}\\right) \\\\\n",
        "    &= \\frac{\\partial}{\\partial w_i}\\left(\\frac{2e\\partial e}{\\partial w_j}\\right) \\\\\n",
        "    &= 2\\frac{\\partial e}{\\partial w_i}\\frac{\\partial e}{\\partial w_j}+2e\\frac{\\partial^2 e}{\\partial w_j \\partial w_i}.\n",
        "\\end{align*}\n",
        "\n",
        "We neglect the second-order derivative term because the term is relatively small ($e$ is small)\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathbf{H}_{ij}\n",
        "    &\\propto \\frac{\\partial e}{\\partial w_i}\\frac{\\partial e}{\\partial w_j},\n",
        "\\end{align*}\n",
        "\n",
        "and as the error $e$ is a constant\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathbf{H}_{ij}\n",
        "    &\\propto \\frac{\\partial L}{\\partial w_i}\\frac{\\partial L}{\\partial w_j},\n",
        "\\end{align*}\n",
        "\n",
        "then the full Hessian becomes\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathbf{H} &\\propto (\\mathbf{B}\\times\\mathbf{A})\\times(\\mathbf{B}\\times\\mathbf{A}).\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1X-2uxwTcB9u"
      },
      "outputs": [],
      "source": [
        "# the main function to compute gradient norm and minimum ratio\n",
        "def main(model, train, target):\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    gradient_norm = compute_gradient_norm(model, criterion, train, target)\n",
        "    minimum_ratio = compute_minimum_ratio(model, criterion, train, target)\n",
        "\n",
        "    print('gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwHyQHc9w8k1"
      },
      "source": [
        "After running this block, you will get the value of `gradient norm` and `minimum ratio`. Determine whether the model is at `local minima like`, `saddle point`, or `none of the above`, and then submit your choice to NTU COOL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "877W_ShIzS7a",
        "outputId": "6c90fdd9-0bbd-405c-c781-457d265c1606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gradient norm: 0.0009448822238482535, minimum ratio: 0.47265625\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # fix random seed\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # reset compute dictionaries\n",
        "    activations = defaultdict(int)\n",
        "    hess = defaultdict(float)\n",
        " \n",
        "    # compute Hessian\n",
        "    main(model, train, target)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SHARE MLSpring2021 - HW2-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
